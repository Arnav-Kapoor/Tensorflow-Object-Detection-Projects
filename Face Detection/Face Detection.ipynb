{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c295ed18-96c7-4c9e-b633-931e92de01f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import uuid\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c2b0bc-d006-46f8-84fe-0ce8c317ae4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install labelme albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77441509-21de-4733-bcbb-d10c8d5cfd08",
   "metadata": {},
   "source": [
    "### collecting images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b074ff-e410-4d4f-87cf-8506a1af24d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH=os.path.join(\"data\",\"images\")\n",
    "num_of_images=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180e202d-25c9-4eba-a462-3398c572f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "for img_num in range(num_of_images):\n",
    "    print(\"Collecting image \",img_num)\n",
    "    ret,frame=cap.read()\n",
    "    imgpath=os.path.join(IMAGES_PATH,f'{str(uuid.uuid1())}.jpg',)\n",
    "    cv2.imwrite(imgpath,frame)\n",
    "    cv2.imshow(\"frame\",frame)\n",
    "    time.sleep(1)\n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6c1cbb-e3a8-44b7-913f-3f057930360a",
   "metadata": {},
   "source": [
    "### annotating images with labelMe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d79111-3584-4b06-82b6-79bf0066e5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!labelme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ab352e-b6e4-480b-a573-252e813e04f8",
   "metadata": {},
   "source": [
    "### Review dataset and build image loading function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713e989f-2864-4014-a664-e3c22421a476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7c72a1-2c20-43fb-a78d-d82bd4d6c0cf",
   "metadata": {},
   "source": [
    "#### load image into tf data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e726c429-a54f-4299-b36d-3e25263a4190",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=tf.data.Dataset.list_files(\"data/images/*.jpg\",shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8e5fc4-5eef-4804-9d92-f9cdf0096302",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cf68a6-893d-46ae-84da-a6218b57782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_img=tf.io.read_file(x)\n",
    "    img=tf.io.decode_jpeg(byte_img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b5c6ce-2f87-4bb9-8a4b-7ba222d5d52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "images=images.map(load_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843da758-edde-4628-8442-162ba400bab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a70e23-7035-414a-94a8-e733e47bde14",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_generator=images.batch(4).as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bddb94-dcd7-40a4-a186-93db976550a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images=image_generator.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e11be2-3cfb-4fe5-a0cd-98653274626f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=4,figsize=(20,20))\n",
    "for idx,image in enumerate(plot_images):\n",
    "    ax[idx].imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96bd133-1915-4c23-a34a-d7b37d1b7642",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### train test split done manually"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e1b1ca-7676-4cf8-bf05-2f08e59b925e",
   "metadata": {},
   "source": [
    "#### moving matching labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772c90a1-5c6c-4d35-9c4c-ac7ca4076578",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folders in ['train','test','val']:\n",
    "    for file in os.listdir(os.path.join('data',folders,'images')):\n",
    "        filename=file.split('.')[0]+'.json'\n",
    "        existing_filepath=os.path.join('data','labels',filename)\n",
    "        if os.path.exists(existing_filepath):\n",
    "            new_filepath=os.path.join('data',folders,'labels',filename)\n",
    "            os.replace(existing_filepath,new_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836058e6-ad50-40d6-abca-b0a317211322",
   "metadata": {},
   "source": [
    "### apply image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55c32f04-3f9c-494f-bc9c-3f4072eb07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as alb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc65f15-6504-4e07-b4c7-bf8de59aa65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = alb.Compose([\n",
    "    alb.RandomCrop(width=450, height=450),\n",
    "    alb.HorizontalFlip(p=0.5),\n",
    "    alb.RandomBrightnessContrast(p=0.2),\n",
    "    alb.RandomGamma(p=0.2),\n",
    "    alb.RGBShift(p=0.2),\n",
    "    alb.VerticalFlip(p=0.5)],\n",
    "    bbox_params=alb.BboxParams(format='albumentations',label_fields=['class_labels']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81abd5a-669e-4aa2-9267-a814516268b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "img=cv2.imread(os.path.join('data','train','images','941bed60-199a-11ee-baac-900f0c7ffb34.jpg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa40b066-14c0-4917-ad88-c981191eec23",
   "metadata": {},
   "outputs": [],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a39c1dd-6aa7-46bc-ae8e-e3edcd7f64c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join('data','train','labels','941bed60-199a-11ee-baac-900f0c7ffb34.json'),'r') as f:\n",
    "    label=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a54a2f-7a17-49b0-8b21-11353aa9c342",
   "metadata": {},
   "outputs": [],
   "source": [
    "label['shapes'][0]['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b928360-237f-45cc-af41-6e87ffac49a6",
   "metadata": {},
   "source": [
    "#### extract coordinates and rescale to match image resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8559fe8c-6dd1-4fb7-8389-e1bfc503fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=[0,0,0,0]\n",
    "coords[0]=label['shapes'][0]['points'][1][0]\n",
    "coords[1]=label['shapes'][0]['points'][1][1]\n",
    "coords[2]=label['shapes'][0]['points'][0][0]\n",
    "coords[3]=label['shapes'][0]['points'][0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d813145-ee81-4008-a9a5-ff9a7464a6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb02370-01d4-4992-97f8-a98f1dabd599",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords=list(np.divide(coords,[640,480,640,480]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58e4172-463b-4a82-9030-df7705c84db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f664a2fa-13c0-4eb2-aa30-6e10979df53d",
   "metadata": {},
   "source": [
    "#### apply augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5a9c9-97e9-4f5d-9d48-4cf894a90bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented=transform(image=img,bboxes=[coords],class_labels=['face'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d862f7-66c1-4ab1-a43b-1af90bc84513",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eab9eb7-c949-478b-90e0-fe38510461aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.rectangle(augmented['image'],tuple(np.multiply(augmented['bboxes'][0][0:2],[450,450]).astype(int)),\n",
    "              tuple(np.multiply(augmented['bboxes'][0][2:],[450,450]).astype(int)), (255,0,0),2)\n",
    "plt.imshow(augmented['image'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f385ee4-6f81-4c81-a08d-360cd9b42649",
   "metadata": {},
   "source": [
    "### augmentation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297ac5ed-a18d-48df-856f-e3b2d7a4ab7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['train','test','val']:\n",
    "    for image in os.listdir(os.path.join('data',folder,'images')):\n",
    "        img =cv2.imread(os.path.join('data',folder,'images',image))\n",
    "        coords=[0,0,0.00001,0.00001]\n",
    "        label_path=os.path.join('data',folder,'labels',f'{image.split(\".\")[0]}.json')\n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path,'r') as f:\n",
    "                label=json.load(f)\n",
    "                \n",
    "            if (label['shapes'][0]['points'][1][0])<(label['shapes'][0]['points'][0][0]):\n",
    "                coords[0]=label['shapes'][0]['points'][1][0]\n",
    "                coords[1]=label['shapes'][0]['points'][1][1]\n",
    "                coords[2]=label['shapes'][0]['points'][0][0]\n",
    "                coords[3]=label['shapes'][0]['points'][0][1]\n",
    "            else:\n",
    "                coords[0]=label['shapes'][0]['points'][0][0]\n",
    "                coords[1]=label['shapes'][0]['points'][0][1]\n",
    "                coords[2]=label['shapes'][0]['points'][1][0]\n",
    "                coords[3]=label['shapes'][0]['points'][1][1]\n",
    "            coords=list(np.divide(coords,[640,480,640,480]))\n",
    "            \n",
    "        for i in range(60):\n",
    "            augmented=transform(image=img,bboxes=[coords],class_labels=['face'])\n",
    "            cv2.imwrite(os.path.join('aug_data',folder,'images',f'{image.split(\".\")[0]}.{i}.jpg'),augmented['image'])\n",
    "            \n",
    "            annotations={}\n",
    "            annotations['image']=image\n",
    "            \n",
    "            if(os.path.exists(label_path)):\n",
    "                if len(augmented['bboxes'])==0:\n",
    "                    annotations['bboxes']=[0,0,0,0]\n",
    "                    annotations['class']=0\n",
    "                else:\n",
    "                    annotations['bboxes']=augmented['bboxes'][0]\n",
    "                    annotations['class']=1\n",
    "            else:\n",
    "                annotations['bboxes']=[0,0,0,0]\n",
    "                annotations['class']=0\n",
    "            \n",
    "            with open(os.path.join('aug_data',folder,'labels',f'{image.split(\".\")[0]}.{i}.json'),'w') as f:\n",
    "                json.dump(annotations,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e6d25-bd7c-4818-998d-d37592d78682",
   "metadata": {},
   "source": [
    "#### load augmented images into tensorlfow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9f9307-e9e0-4da7-bb97-9805d4df1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = tf.data.Dataset.list_files('aug_data/train/images/*.jpg', shuffle=False)\n",
    "train_images = train_images.map(load_image)\n",
    "train_images = train_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "train_images = train_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883d4f7-ba6c-4b10-9818-29bad4b1e1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = tf.data.Dataset.list_files('aug_data/test/images/*.jpg', shuffle=False)\n",
    "test_images = test_images.map(load_image)\n",
    "test_images = test_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "test_images = test_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9d048a-815b-4c3f-8a44-aec03f581d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = tf.data.Dataset.list_files('aug_data/val/images/*.jpg', shuffle=False)\n",
    "val_images = val_images.map(load_image)\n",
    "val_images = val_images.map(lambda x: tf.image.resize(x, (120,120)))\n",
    "val_images = val_images.map(lambda x: x/255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e12d946-6f9d-4c02-b964-b17c6c078df2",
   "metadata": {},
   "source": [
    "### preparing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1059eb-c1ec-4493-87ca-c375713eda0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_labels(label_path):\n",
    "    with open(label_path.numpy(),'r',encoding='utf-8') as f:\n",
    "        label=json.load(f)\n",
    "    \n",
    "    return [label['class'],label['bboxes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3735e650-b635-4660-b5ba-6fc63fd6305d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = tf.data.Dataset.list_files('aug_data/train/labels/*.json', shuffle=False)\n",
    "train_labels = train_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b3ad1-3b36-4713-84e4-de1423d5cddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = tf.data.Dataset.list_files('aug_data/test/labels/*.json', shuffle=False)\n",
    "test_labels = test_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce456264-eff5-4113-a645-f303524ff5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = tf.data.Dataset.list_files('aug_data/val/labels/*.json', shuffle=False)\n",
    "val_labels = val_labels.map(lambda x: tf.py_function(load_labels, [x], [tf.uint8, tf.float16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a008c0f4-cac9-4839-ba25-25ab00ae28e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48840d5e-201e-4229-858b-df2b8768269c",
   "metadata": {},
   "source": [
    "### combine image and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2328cb1f-f4ca-4f0f-ad3b-92187e7c99a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_images),len(train_labels),len(test_images),len(test_labels),len(val_images),len(val_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eed01d3d-b043-4d0f-97d7-0b2512a8779c",
   "metadata": {},
   "source": [
    "#### create final datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993d36d8-9e9d-4ab1-ad71-3104360cd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tf.data.Dataset.zip((train_images,train_labels))\n",
    "train=train.shuffle(3000)\n",
    "train=train.batch(8)\n",
    "train=train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70064c32-c255-48bc-bd51-9ccaff77c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=tf.data.Dataset.zip((test_images,test_labels))\n",
    "test=test.shuffle(3000)\n",
    "test=test.batch(8)\n",
    "test=test.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0a2231-69e8-4898-946b-7594ddced457",
   "metadata": {},
   "outputs": [],
   "source": [
    "val=tf.data.Dataset.zip((val_images,val_labels))\n",
    "val=val.shuffle(3000)\n",
    "val=val.batch(8)\n",
    "val=val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44efe815-ecf9-46be-bb9b-a65daa7c20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.as_numpy_iterator().next()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99afd11b-63ef-492c-b5ca-58998c989be1",
   "metadata": {},
   "source": [
    "#### view images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29cb9f-c621-4610-845c-2c02874a0623",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_samples=train.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dea9b9-d5c2-4705-b0be-c436bd8f122b",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=data_samples.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0539d52-0956-4c70-8021-92861268ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=4,figsize=(120,120))\n",
    "for idx in range(4):\n",
    "    sample_image=res[0][idx]\n",
    "    sample_coords=res[1][1][idx]\n",
    "    \n",
    "    cv2.rectangle(sample_image,\n",
    "                  tuple(np.multiply(sample_coords[:2],[120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:],[120,120]).astype(int)),\n",
    "                 (255,0,0),2)\n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f7e6fd-9c9e-403b-8dca-2943da722ec3",
   "metadata": {},
   "source": [
    "### Build deep learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d54a99-a037-4196-9ccb-9d40a6463338",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, GlobalMaxPooling2D, Dense\n",
    "from tensorflow.keras.applications import VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68be5860-39bf-4918-8735-5cd365dd8cdc",
   "metadata": {},
   "source": [
    "#### download vgg16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846aab8e-ac9f-4ca6-8b62-99efc511e69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg=VGG16(include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52194af-6080-45f9-b69a-056d90869a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ed1585-d8e9-43fe-bf88-a7ba367c26f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    input_layer=Input(shape=(120,120,3))\n",
    "    vgg=VGG16(include_top=False)(input_layer)\n",
    "    \n",
    "    #classification model\n",
    "    f1=GlobalMaxPooling2D()(vgg)\n",
    "    class1=Dense(units=2048,activation='relu')(f1)\n",
    "    class2=Dense(units=1,activation='sigmoid')(class1)\n",
    "    \n",
    "    #bounding box model\n",
    "    f2=GlobalMaxPooling2D()(vgg)\n",
    "    regres1=Dense(units=2048,activation='relu')(f2)\n",
    "    regres2=Dense(4,activation='sigmoid')(regres1)\n",
    "    \n",
    "    facetracker=Model(inputs=input_layer,outputs=[class2,regres2])\n",
    "    \n",
    "    return facetracker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106179b3-d405-45c9-83ba-2fcd3eec5514",
   "metadata": {},
   "source": [
    "#### test neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4320892e-2506-459a-8dff-ca82e8f90af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker=build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a53812c-9d97-4a2d-8d20-fa64556c5e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72af70f5-e476-43bb-9b6e-c24616de0944",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=train.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173fb1b0-649c-4e8d-8bcc-61cce584bdfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eee2ba-07b0-4332-854f-57b37b5abf10",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3a3ea1-e047-45f1-a4fc-23a14ade20be",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes,coords=facetracker.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81687267-9c0a-471d-919e-c58843866a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes,coords ##not correct as nn is not trained"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4781b852-f8e4-4e66-87e0-34d45e8caf15",
   "metadata": {},
   "source": [
    "### Define losses and optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4178c62-808a-461a-bc0b-1d0cd2925d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea498-395a-412e-806c-165ec34df047",
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch=len(train)\n",
    "lr_decay=(1./0.75-1)/batches_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8896c593-df02-4209-84a8-2774ed2ff39c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt=tf.keras.optimizers.legacy.Adam(learning_rate=0.0001,decay=lr_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d256df-109c-4a85-8ee9-bc617a759dd8",
   "metadata": {},
   "source": [
    "#### create localisation loss and classification loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8cc8d-d524-4426-ac04-e5e08863b47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def localizaton_loss(y_true,yhat):\n",
    "    delta_coord=tf.reduce_sum(tf.square(y_true[:,:2]-yhat[:,:2]))\n",
    "    \n",
    "    h_true=y_true[:,3]-y_true[:,1]\n",
    "    w_true=y_true[:,2]-y_true[:,0]\n",
    "    \n",
    "    h_pred=yhat[:,3]-yhat[:,1]\n",
    "    w_pred=yhat[:,2]-yhat[:,0]\n",
    "    \n",
    "    delta_size=tf.reduce_sum(tf.square(h_true-h_pred)+tf.square(w_true-w_pred))\n",
    "    \n",
    "    return delta_size+delta_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b2e801-2487-4384-8f46-7b16a5cd2098",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss=tf.keras.losses.BinaryCrossentropy()\n",
    "regressloss=localizaton_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ad61ac-9732-4a73-b2d3-9a778f8cefc7",
   "metadata": {},
   "source": [
    "#### test loss metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08425260-aee8-454c-8c48-925674df5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "localizaton_loss(y[1],coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86592055-0b6e-453a-b248-170ed6b2f5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "classloss(y[0],classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9565ab-aa0d-4b1d-a044-967b373eea1b",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb171819-d623-4438-9375-1631324be6b2",
   "metadata": {},
   "source": [
    "#### create custom model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469b3870-3c99-409e-a05b-d66fca7fcac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FaceTracker(Model):\n",
    "    def __init__(self,facetracker,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model=facetracker\n",
    "    \n",
    "    def compile(self,opt,classloss,localizatonloss,**kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.closs=classloss\n",
    "        self.lloss=localizatonloss\n",
    "        self.opt=opt\n",
    "        \n",
    "    def train_step(self,batch,**kwargs):\n",
    "        X,y=batch\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            classes,coords=self.model(X,training=True)\n",
    "            \n",
    "            batch_classloss=self.closs(y[0],classes)\n",
    "            batch_localizationloss=self.lloss(tf.cast(y[1],tf.float32),coords)\n",
    "            \n",
    "            total_loss=batch_localizationloss+0.5*batch_classloss\n",
    "            \n",
    "            grad=tape.gradient(total_loss,self.model.trainable_variables)\n",
    "        \n",
    "        opt.apply_gradients(zip(grad,self.model.trainable_variables))\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regess_loss\":batch_localizationloss}\n",
    "    \n",
    "    def test_step(self,batch,**kwargs):\n",
    "        X,y=batch\n",
    "        \n",
    "        classes,coords=self.model(X,training=False)\n",
    "        \n",
    "        batch_classloss=self.closs(y[0],classes)\n",
    "        batch_localizationloss=self.lloss(tf.cast(y[1],tf.float32),coords)\n",
    "        \n",
    "        total_loss=batch_localizationloss+0.5*batch_classloss\n",
    "        \n",
    "        return {\"total_loss\":total_loss, \"class_loss\":batch_classloss, \"regess_loss\":batch_localizationloss}\n",
    "    \n",
    "    def call(self,X,**kwargs):\n",
    "        return self.model(X,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8505e80-b8f7-4d30-bc9a-d21e3c3ef206",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FaceTracker(facetracker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db14265d-935d-4233-99ae-53d079f8f2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt,classloss,regressloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b316f8e2-4349-40b8-8705-952a89a09a6e",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a27ee7-8436-404a-ae66-5a6e3addf2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "logdir='logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6671c8-79c0-4eb7-9224-6315bad51881",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard_callbacks=tf.keras.callbacks.TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714c474-7408-4d86-a791-ee551242d41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist=model.fit(train,epochs=13,validation_data=val,callbacks=[tensorboard_callbacks])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c109e-e564-44ad-b99d-4ddf3f63a328",
   "metadata": {},
   "source": [
    "#### plot performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193ab1bf-1c50-4507-9d22-bfdd30b157e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=3,figsize=(20,5))\n",
    "\n",
    "ax[0].plot(hist.history['total_loss'],color='teal',label='loss')\n",
    "ax[0].plot(hist.history['val_total_loss'],color='orange',label='val_loss')\n",
    "ax[0].title.set_text('loss')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].plot(hist.history['class_loss'],color='teal',label='class_loss')\n",
    "ax[1].plot(hist.history['val_class_loss'],color='orange',label='val_class_loss')\n",
    "ax[1].title.set_text('class loss')\n",
    "ax[1].legend()\n",
    "\n",
    "ax[2].plot(hist.history['regess_loss'],color='teal',label='regress_loss')\n",
    "ax[2].plot(hist.history['val_regess_loss'],color='orange',label='val_regress_loss')\n",
    "ax[2].title.set_text('regress loss')\n",
    "ax[2].legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d292ff6-0fd2-4ad6-908c-54d5dbc31c07",
   "metadata": {},
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a9403-3f3e-4e35-83a3-d458a15fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test.as_numpy_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3c1dfd-a19d-4b09-a228-6bc60ebb430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample=test_data.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13714824-1224-4348-ac62-6b2a0e79b6ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat=facetracker.predict(test_sample[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f002fd-53d9-48fd-85e6-4b30a686ec24",
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994d593f-136a-45b9-a240-3c943e73466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots(ncols=4,figsize=(20,20))\n",
    "\n",
    "for idx in range(4):\n",
    "    sample_image=test_sample[0][idx]\n",
    "    sample_coords=yhat[1][idx]\n",
    "    \n",
    "    if yhat[0][idx]>0.5:\n",
    "        cv2.rectangle(sample_image,\n",
    "                  tuple(np.multiply(sample_coords[:2],[120,120]).astype(int)),\n",
    "                  tuple(np.multiply(sample_coords[2:],[120,120]).astype(int)),\n",
    "                 (255,0,0),2)\n",
    "        \n",
    "    ax[idx].imshow(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95668551-d3f2-47fb-b6d1-253cada0b416",
   "metadata": {},
   "source": [
    "#### save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e84d7b-2e71-47ed-86af-0c60ed1d5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67ca5b0-7375-47fa-aaad-2ffa9436cceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker.save('facetracker.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5016b608-099a-412f-ab0e-adefe7e0b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "facetracker_loaded=load_model('facetracker.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b631882-a1cc-4a8f-b646-93539a8fa4a7",
   "metadata": {},
   "source": [
    "#### realtime detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f45106-ef48-45c7-aae6-ce1387a90c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret,frame=cap.read()\n",
    "    frame=frame[50:500,50:500,:]\n",
    "    \n",
    "    rgb=cv2.cvtColor(frame,cv2.COLOR_BGR2RGB)\n",
    "    resized=tf.image.resize(rgb,(120,120))\n",
    "    yhat=facetracker_loaded.predict(np.expand_dims(resized/255,0))\n",
    "    coords=yhat[1][0]\n",
    "    \n",
    "    if yhat[0][0]>0.5:\n",
    "        print(yhat)\n",
    "        ### main rectangle\n",
    "        cv2.rectangle(frame,tuple(np.multiply(coords[:2],[450,450]).astype(int)),\n",
    "                      tuple(np.multiply(coords[2:],[450,450]).astype(int)),\n",
    "                      (255,0,0),2)\n",
    "        \n",
    "        ### text rectangle\n",
    "        cv2.rectangle(frame, \n",
    "                      tuple(np.add(np.multiply(coords[:2], [450,450]).astype(int), \n",
    "                                    [0,-30])),\n",
    "                      tuple(np.add(np.multiply(coords[:2], [450,450]).astype(int),\n",
    "                                    [80,0])), \n",
    "                            (255,0,0), -1)\n",
    "        \n",
    "        ### text\n",
    "        cv2.putText(frame, 'face', tuple(np.add(np.multiply(coords[:2], [450,450]).astype(int),\n",
    "                                               [0,-5])),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "        \n",
    "    cv2.imshow(\"facetracker\",frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3af281-10fc-49d6-9373-0e2a1280b8d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfod",
   "language": "python",
   "name": "tfod"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
