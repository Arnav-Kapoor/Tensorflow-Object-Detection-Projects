{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47496e15-8fde-452b-be74-2033fbdee5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### THIS PROJECT USES TENSORFLOW=2.15 AND KERAS=3.0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fc3b2d-3806-4e0d-b010-f14470053d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc291c3-6fb9-468e-817f-e5b3d3ef5e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade=cv2.CascadeClassifier('haarcascade_frontalface_default.xml')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f4be38a-85ab-42ca-a217-b99826d127b6",
   "metadata": {},
   "source": [
    "#### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fbcbc4-3afe-49c0-ab9e-7a750240f721",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap=cv2.VideoCapture(0)\n",
    "while True:\n",
    "    ret,frame=cap.read()\n",
    "    if ret:\n",
    "        # cv2.rectangle(frame,(140,50),(500,400),(0,0,255),2)\n",
    "        gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "        faces=face_cascade.detectMultiScale(gray,scaleFactor=1.3,minNeighbors=3)\n",
    "        # a=0,b=0,c=0,d=0\n",
    "        for (x,y,w,h) in faces:\n",
    "            cv2.rectangle(frame,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "            # a=x,b=y,c=w,d=h\n",
    "        cv2.imshow(\"test\",frame[140:500,50:400])\n",
    "        if cv2.waitKey(1) & 0xFF==ord('q'):\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b5023f-a90f-48ee-8e7c-e6167a875f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090bbc3-7457-4b6e-b9ae-faefcd8caa1f",
   "metadata": {},
   "source": [
    "### Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67788695-9b0f-4d78-9ce0-b11d665f68c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img in os.listdir(\"images\"):\n",
    "    image=cv2.imread(os.path.join(\"images\",img))\n",
    "    name=img\n",
    "    new_img=tf.image.stateless_random_flip_up_down(image,seed=(np.random.randint(100),np.random.randint(100)))\n",
    "    new_img = tf.image.stateless_random_flip_left_right(new_img, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "    new_img = tf.image.stateless_random_brightness(new_img, max_delta=0.02, seed=(1,2))\n",
    "    new_img = tf.image.stateless_random_contrast(new_img, lower=0.6, upper=1, seed=(1,3))\n",
    "    new_img = tf.image.stateless_random_jpeg_quality(new_img, min_jpeg_quality=90, max_jpeg_quality=100, seed=(np.random.randint(100),np.random.randint(100)))\n",
    "    new_img = tf.image.stateless_random_saturation(new_img, lower=0.9, upper=1, seed=(np.random.randint(100),np.random.randint(100))) \n",
    "    cv2.imwrite(os.path.join(\"images\",name),new_img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7009696",
   "metadata": {},
   "source": [
    "Creating tf data pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc22e4fe-7abd-40b3-968f-d01235a6f471",
   "metadata": {},
   "outputs": [],
   "source": [
    "positives=tf.data.Dataset.from_tensor_slices(tf.io.gfile.glob(\"images/*.jpg\")[:5600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff146bc7-5eb0-4011-a952-7cefeda0ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd967f26-3c10-4407-aae0-bba22558bc49",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives=tf.data.Dataset.from_tensor_slices(tf.io.gfile.glob(\"images/no face/*.jpg\")[:4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04acfd48-d934-44c1-8003-fa74a799fa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992e9d11-8394-41ce-81f1-0e21cc3823fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfca7e82-4f00-4e17-a64c-deaa09fc55e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive=tf.data.Dataset.zip((positives,tf.data.Dataset.from_tensor_slices(tf.ones(len(positives)))))\n",
    "negative=tf.data.Dataset.zip((negatives,tf.data.Dataset.from_tensor_slices(tf.zeros(len(negatives)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1886db-64ae-420a-9518-aed864df12fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive,negative.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059d3f32-3813-4992-9dcd-5800b31e066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(positive),len(negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85ab4b0",
   "metadata": {},
   "source": [
    "Preprocessing image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad6fb97-bc19-4c6a-8959-c671ae3cf229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(x):\n",
    "    byte_image=tf.io.read_file(x)\n",
    "    img=tf.io.decode_jpeg(byte_image,channels=3)\n",
    "    img=tf.image.resize(img,(200,200))\n",
    "    img=img/255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac01ce90-994f-4b14-b772-d2c31fcdf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=positive.concatenate(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a852ba-6c8b-40ac-990d-0ef1502fca43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4931d081-abd0-4ee1-98e4-fb73f7a1261a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=data.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a9e9ef-8613-40f6-8a78-0d3cf0f0043b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a2563-43fd-4e37-b9e2-910395e03f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(image,label):\n",
    "    return(load_image(image),label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc5812b-83ed-4abe-b9f6-7c742c2d34ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "res=preprocess(*sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babec730-41c0-4378-b3f4-20306537baaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728fdc89",
   "metadata": {},
   "source": [
    "Building data loader pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3feb48-8d8e-46f2-b7c6-7c18d62e38c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.map(preprocess)\n",
    "data=data.cache()\n",
    "data=data.shuffle(buffer_size=15000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8eeb029-696c-4322-9c50-f71c655624e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train=data.take(round(len(data)*.7))\n",
    "train=train.batch(8)\n",
    "train=train.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0950ff-2adc-4a1e-9477-cb4760d06d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "val=data.skip(round(len(data)*.7))\n",
    "val=val.take(round(len(data)*.3))\n",
    "val=val.batch(8)\n",
    "val=val.prefetch(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f36f82-cf46-407b-bd30-26428219270a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train),len(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1c4f27-44e3-4ce7-8f3d-9573980b2565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.element_spec,val.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503d0570-e586-4726-b6dd-296142460f3c",
   "metadata": {},
   "source": [
    "### building Feature Extractor model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f34f7e3-f693-444d-a3d9-5c21666a4e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv2D,Layer,Input,Dense,MaxPooling2D,Flatten,Concatenate,Normalization\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.regularizers import L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9024c0-bbdb-4b7b-8f01-2ada32dd71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FeatureExtractor():\n",
    "    i=Input(shape=(200,200,3))\n",
    "    c1=Conv2D(64,kernel_size=(5,5),strides=(2,2),activation=\"relu\",name=\"first_conv\")(i)\n",
    "    m1=MaxPooling2D(pool_size=(2,2),strides=(1,1),padding=\"same\")(c1)\n",
    "    \n",
    "    c2=Conv2D(32,kernel_size=(3,3),strides=(2,2),activation=\"relu\",name=\"second_conv\")(m1)\n",
    "    m2=MaxPooling2D(pool_size=(5,5),strides=(2,2),padding=\"same\")(c2)\n",
    "    \n",
    "    c3=Conv2D(128,kernel_size=(5,5),strides=(2,2),activation=\"relu\",name=\"feature_extractor\")(m2)\n",
    "    \n",
    "    f=Flatten()(c3)\n",
    "    \n",
    "    x=Dense(16,activation='relu',kernel_regularizer=L2())(f)\n",
    "\n",
    "    return [i,x,c3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796b6fd0-a8cd-4bf1-9157-ba3d994701c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EuclideanDistance(feature1,feature2):\n",
    "    return np.linalg.norm(feature1-feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a3c1ae-0d82-42c4-a098-4b16a9103304",
   "metadata": {},
   "outputs": [],
   "source": [
    "## last fully connected dense layer\n",
    "def addLastLayer():\n",
    "    i,fully_connected,c3=FeatureExtractor() #input layer, second last fully connected dense layer, feature extractor layer\n",
    "    x=Dense(1,activation=\"sigmoid\")(fully_connected)\n",
    "    final_model=Model(inputs=[i],outputs=[x,c3]) #outputs class and features\n",
    "    final_model.build(input_shape=(200,200,3))\n",
    "    return final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337306b0-2fb6-4204-b82f-dae69478b215",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model=addLastLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ba554e-19ce-46eb-8057-031c5f11fa6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07d4e9a-7dd4-4797-8b3b-1e5cee6af0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss=tf.keras.losses.binary_crossentropy\n",
    "accuracy=tf.keras.metrics.BinaryAccuracy(name=\"accuracy\")\n",
    "opt=tf.keras.optimizers.Adam(learning_rate=3e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b66e3b2",
   "metadata": {},
   "source": [
    "creating subclass model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3de10a-73ce-44a2-bf5e-eb92c9b3de6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attendance(Model):\n",
    "    def __init__(self,model,**kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.model=model\n",
    "        \n",
    "    def compile(self,opt,loss,metric,**kwargs):\n",
    "        super().compile(**kwargs)\n",
    "        self.loss=loss\n",
    "        self.opt=opt\n",
    "        self.metric=metric\n",
    "    \n",
    "    def train_step(self,batch,**kwargs):\n",
    "        X,y=batch\n",
    "        y=tf.expand_dims(y, axis=-1)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred=self.model(X,training=True)\n",
    "            classLoss=self.loss(y,y_pred[0])\n",
    "            grad=tape.gradient(classLoss,self.model.trainable_variables)\n",
    "        opt.apply_gradients(zip(grad,self.model.trainable_variables))\n",
    "        \n",
    "        acc=accuracy(y,y_pred[0])\n",
    "        return {\"loss\":classLoss,\"accuracy\":acc}\n",
    "    \n",
    "    def test_step(self,batch,**kwargs):\n",
    "        X,y=batch\n",
    "        y=tf.expand_dims(y, axis=-1)\n",
    "        y_pred=self.model(X,training=False)\n",
    "        vaLoss=self.loss(y,y_pred[0])\n",
    "        self.metric.update_state(y, y_pred[0])\n",
    "        return {\"loss\":vaLoss,\"accuracy\":self.metric.result()}\n",
    "    \n",
    "    def get_config(self):\n",
    "        base_config = super().get_config()\n",
    "        config = {\n",
    "                \"submodel\": tf.keras.utils.serialize_keras_object(self.model),\n",
    "                }\n",
    "        return {**base_config, **config}\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        submodel_config = config.pop(\"submodel\")\n",
    "        submodel = tf.keras.utils.deserialize_keras_object(submodel_config)\n",
    "        return cls(model=submodel, **config)\n",
    "    \n",
    "    def call(self,X,**kwargs):\n",
    "        return self.model(X,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08522b47-a090-4256-9212-a7180d217c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Attendance(final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e9163ef-9c5b-41f4-8bdb-883a4ff0d047",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(opt=opt,loss=loss,metric=accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa67021f-bc81-464e-84ed-2c936f3acaa6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "hist=model.fit(train,epochs=5,validation_data=val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa54d4fc-e0e7-466f-8862-fa83e7e9e2cc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.predict(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358baea0-fcc1-45a9-9018-9755bf2ab110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"AttendanceSystem.keras\")\n",
    "# model.save_weights(\"model_weights.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e183cdb",
   "metadata": {},
   "source": [
    "importing trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c1de80-d9a9-4e15-a1a1-036d87800e37",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import custom_object_scope\n",
    "\n",
    "with custom_object_scope({'Attendance': Attendance}):\n",
    "    # custom_objects = {'Attendance': Attendance.from_config}\n",
    "    # custom_objects={'Functional':tf.keras.models.Model}\n",
    "    model=tf.keras.models.load_model(\"AttendanceSystem.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38481f60-1524-416f-87f0-8ef1695c625e",
   "metadata": {},
   "source": [
    "### Manual Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dc185b-71fb-4c2d-aec7-4f7cbcc0fa53",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=os.path.join(\"images\",\"Zinedine_Zidane_0001.jpg\")\n",
    "# test_img=os.path.join(\"D:\\downloads\\IMG_20220113_172032 (1) (2022_07_10 07_51_22 UTC).jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148c87ab-1fa6-43fc-a6db-77e727dbcc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae2ca55-4e0a-4c20-9c0c-4b961709f3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=load_image(test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344d58d7-dafe-4d93-8e6d-a4e27a03cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_img=np.expand_dims(test_img,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c90ba66-81db-4663-81c1-96d931b5d910",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c27dd48-6ab6-4d3d-82f8-f9645306ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature1=model.predict(test_img)[1]\n",
    "# feature1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416b25a2-c189-4e4c-b424-3e2868522425",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature2=model.predict(np.expand_dims(load_image(os.path.join(\"images\",\"Zydrunas_Ilgauskas_0001.jpg\")),axis=0))[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac373468-f550-4e30-8f0e-11284b830b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1=EuclideanDistance(feature1,feature2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a54d1099-6cfe-439d-a4bd-7a63dd6e620f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2=EuclideanDistance(feature1,model.predict(np.expand_dims(load_image(os.path.join(\"images\",\"Zinedine_Zidane_0004.jpg\")),axis=0))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c23dccb-39f9-4b4f-9a82-5d0e4a92b47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist1,dist2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8c306d-7430-400c-8aa5-0850dac1ea47",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist2<dist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726e7e3-ab5c-4a69-b069-114a356f3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_feature={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e33ab-ef2e-42bc-96db-fba431a911f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for peeps in ['Zinedine_Zidane_0002', 'Zydrunas_Ilgauskas_0001', 'Zoe_Ball_0001', 'Zico_0001', 'Yuri_Malenchenko_0002', 'Zoran_Djindjic_0003']:\n",
    "    person_feature[peeps]=EuclideanDistance(feature1,model.predict(np.expand_dims(load_image(os.path.join(\"images\",peeps+\".jpg\")),axis=0))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "152c2f23-3b6a-45de-9d80-2b228965ab53",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(person_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e8e199-a93a-467a-a89c-3b809d0d0a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance=min(person_feature.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fd49fb-da32-4d5a-a30a-de02e01bd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9d7e73-2690-41d2-9049-8e29f7c7cd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_feature.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e840828-4ccb-4a76-a0fe-b2d2e4c15766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in list(person_feature.keys()):\n",
    "    if person_feature[key]==min_distance:\n",
    "        print(key)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37ae5a6-08dd-4bab-b548-29abebfee297",
   "metadata": {},
   "outputs": [],
   "source": [
    "person_feature.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5704d5bf-2b13-4522-9501-12829c8e7eb0",
   "metadata": {},
   "source": [
    "### Realtime Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b04606-def6-4a87-b637-dc91f67bdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isFace(gray,frame):\n",
    "    faces=face_cascade.detectMultiScale(gray,scaleFactor=1.3,minNeighbors=3)\n",
    "    if len(faces)>0:\n",
    "        return (True,faces) #returns coordinates of face detected\n",
    "    else:\n",
    "        return(False,faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980655f3-f4cb-45a2-93f1-60303b84222a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NewFace():\n",
    "    cap=cv2.VideoCapture(0)\n",
    "    try:\n",
    "        while True:\n",
    "            ret,frame=cap.read()\n",
    "            if ret:\n",
    "                # cv2.rectangle(frame,(140,50),(500,400),(0,0,255),2)\n",
    "                gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "                face,coord=isFace(gray,frame)\n",
    "                # print(coord)\n",
    "                if face:\n",
    "                    for (x,y,w,h) in coord:\n",
    "                        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "                cv2.imshow(\"test\",frame)\n",
    "                key=cv2.waitKey(1) & 0xFF\n",
    "                if face and key==ord('c'):\n",
    "                    name=input(\"Enter name: \")\n",
    "                    print(\"processing..\")\n",
    "                    imgpath=os.path.join(\"user_imgs\",name+\".jpg\")\n",
    "                    #frame[y:y+h,x:x+w]\n",
    "                    cv2.imwrite(imgpath,frame[coord[0][1]:coord[0][1]+coord[0][3],coord[0][0]:coord[0][0]+coord[0][2]]) #saves only the face and not surroundings\n",
    "                    frame=np.expand_dims(load_image(imgpath),axis=0)\n",
    "                    features=model.predict(frame)\n",
    "                    print(features[0])\n",
    "                    np.save(\"features/\"+name+\".npy\",features[1])\n",
    "                    print(\"done\")\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    return\n",
    "                elif key==ord('q'):\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(e,frame.shape)\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77db9685-a50b-44a5-a9ab-11dab678e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NewFace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496e446a-7fb9-4265-9e68-5af0026a977f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SameFaceCheck():\n",
    "    cap=cv2.VideoCapture(0)\n",
    "    try:\n",
    "        while True:\n",
    "            ret,frame=cap.read()\n",
    "            if ret:\n",
    "                gray=cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
    "                face,coord=isFace(gray,frame)\n",
    "                if face:\n",
    "                    for (x,y,w,h) in coord:\n",
    "                        cv2.rectangle(frame,(x,y),(x+w,y+h),(255,255,255),2)\n",
    "                cv2.imshow(\"test\",frame)\n",
    "                key=cv2.waitKey(1) & 0xFF\n",
    "                if face and key==ord('a'):\n",
    "                    print(\"processing..\")\n",
    "                    imgpath=os.path.join(\"user_imgs\",\"temp\"+\".jpg\")\n",
    "                    #frame[y:y+h,x:x+w]\n",
    "                    cv2.imwrite(imgpath,frame[coord[0][1]:coord[0][1]+coord[0][3],coord[0][0]:coord[0][0]+coord[0][2]]) #saves only the face and not surroundings\n",
    "                    img=np.expand_dims(load_image(imgpath),axis=0)\n",
    "                    new=model.predict(img)[1]\n",
    "                    person_feature={}\n",
    "                    #traverses through all the features saved \n",
    "                    for feature in os.listdir(\"features\"):\n",
    "                        present=np.load(\"features/\"+feature)\n",
    "                        distance=EuclideanDistance(present,new)\n",
    "                        person_feature[distance]=feature\n",
    "                    min_dist=min(person_feature.keys())\n",
    "                    print(person_feature[min_dist][:-4])\n",
    "                    os.remove(os.path.join(\"user_imgs\",\"temp.jpg\"))\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    return\n",
    "                elif key==ord('q'):\n",
    "                    break\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6a94d8-1614-453c-b3be-8339cfef9783",
   "metadata": {},
   "outputs": [],
   "source": [
    "SameFaceCheck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208fe920-7c14-4d36-9b8b-d77707b666ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
